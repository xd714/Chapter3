
## **1. Scaling Factor and Matrix Compatibility**

You're absolutely right! The scaling between A and G matrices is **critical** and this is actually one of the most technical challenges in ssGBLUP.

### **Why Scaling Matters**

The A matrix and G matrix measure relationships on different scales:

```
A matrix (pedigree): 
- Parent-offspring = exactly 0.5
- Full siblings = exactly 0.5
- Based on expected relationships

G matrix (genomic):
- Parent-offspring = ~0.5 ± random variation
- Full siblings = 0.35 to 0.65 (due to Mendelian sampling)
- Based on actual DNA sharing
```

### **The Scaling Problem**

Without proper scaling:

```
A says: "These siblings share 50% genetics"
G says: "These siblings share 43% genetics"
→ The matrices contradict each other!
→ Information borrowing becomes inaccurate
```

### **How They Solve It**

The paper mentions they use the **VanRaden et al. (2009)** method to make G compatible with A₂₂:

#### **Step 1: Standardize G matrix**

```
G = ZZ'/k
where:
- Z = centered genotype matrix (subtract 2p from each marker)
- p = allele frequency
- k = 2Σp(1-p) = normalization constant
```

#### **Step 2: Force compatibility**

```
G* = G × (trace(A₂₂)/trace(G)) + α(A₂₂ - G)
```

Where:

- **trace(A₂₂)/trace(G)** = scaling factor to match variances
- **α** = blending parameter (usually small, like 0.05-0.10)
- This forces the average relationship in G to match A₂₂

#### **Step 3: The λ scaling factor**

```
λ = σ²ₑ/σ²ₐ = (1-h²)/h²
```

This scales the relationship matrix relative to the error variance. If heritability h² = 0.5, then λ = 1, meaning genetic and environmental variances are equal.

### **Why This Actually Works**

The key insight is that we're not trying to make the matrices identical, but **compatible for information flow**:

```
Before scaling:
Ungenotyped animal → (wrong scale) → Genotyped relative
Information gets distorted

After scaling:
Ungenotyped animal → (correct scale) → Genotyped relative  
Information flows accurately
```

You're right that if scaling is wrong, the "borrowing" becomes inaccurate. That's why this preprocessing step is crucial.

## **2. BayesA and BayesB: Why "Particular Genes Important"**

Let me clarify this - I think I oversimplified earlier. Here's what BayesA/B actually assume:

### **BayesA: Variable Effect Sizes**

```
Each marker j has effect: βⱼ ~ N(0, σ²βⱼ)
Each marker gets its own variance: σ²βⱼ ~ Scaled-Inverse-χ²(νβ, Sβ)
```

**Key point**: ALL markers have effects, but with **different variances**

### **BayesB: Mixture Model**

```
For each marker j:
- With probability π: βⱼ = 0 (no effect)  
- With probability (1-π): βⱼ ~ N(0, σ²βⱼ)
```

**Key point**: Most markers have NO effect, few markers have large effects

### **Why This Philosophy Matters**

#### **ssGBLUP assumption:**

```
All markers equally important:
β₁ ~ N(0, σ²β)
β₂ ~ N(0, σ²β)  
β₃ ~ N(0, σ²β)
...
Same variance for all!
```

#### **BayesA assumption:**

```
Different importance levels:
β₁ ~ N(0, σ²β₁)  ← maybe small variance
β₂ ~ N(0, σ²β₂)  ← maybe large variance  
β₃ ~ N(0, σ²β₃)  ← maybe medium variance
...
Each gets its own variance!
```

#### **BayesB assumption:**

```
Most genes irrelevant:
β₁ = 0                    ← no effect (90% of genes)
β₂ ~ N(0, σ²β₂)          ← has effect (10% of genes)
β₃ = 0                    ← no effect
...
Sparse effects!
```

### **Which Assumption is More Realistic?**

This depends on the **genetic architecture** of the trait:

#### **For highly polygenic traits** (like height, milk yield):

- Thousands of genes each contribute tiny effects
- ssGBLUP assumption is more realistic
- BayesA/B might over-shrink small effects

#### **For oligogenic traits** (controlled by few major genes):

- Few genes have large effects, most have zero effect
- BayesB assumption is more realistic
- ssGBLUP might under-shrink noise markers

### **Why Authors Compare These**

The authors want to show that:

1. **ssGBLUP is more practical** - works with ungenotyped animals
2. **Performance is comparable** - doesn't sacrifice accuracy for practicality
3. **ssGBLUP is more robust** - lower standard deviations across replicates

## **3. The Information Borrowing Accuracy**

You raise a great point about accuracy. The "borrowing" is only accurate if:

### **Assumptions Hold:**

1. **Relationships are correct** (pedigree errors destroy accuracy)
2. **Scaling is appropriate** (what we discussed above)
3. **Genetic architecture assumptions are reasonable** (infinitesimal model for ssGBLUP)
4. **Population structure is accounted for** (no hidden population stratification)

### **When Borrowing Fails:**

```
Example of failure:
- Animal A (genotyped): high performance due to good management
- Animal B (ungenotyped): related to A, but in poor environment
- Algorithm incorrectly concludes: A has good genes → B should also perform well
- Reality: A's performance was environmental, not genetic
```

### **How They Mitigate This:**

1. **Mixed model structure** separates genetic from environmental effects
2. **Relationship matrix** weights information by actual genetic similarity
3. **Shrinkage** (through λ) prevents over-confidence in noisy data
4. **Cross-validation** in their simulations tests real predictive accuracy

## **Bottom Line**

You've identified the key technical challenges:

1. **Matrix scaling** - absolutely critical and often poorly explained
2. **Information borrowing accuracy** - depends on correct modeling assumptions
3. **Genetic architecture assumptions** - different methods work better for different trait types

The ssGBLUP method works well when these technical details are handled correctly, but you're right that it's not automatically accurate just because it uses more data. The math has to be set up properly first!