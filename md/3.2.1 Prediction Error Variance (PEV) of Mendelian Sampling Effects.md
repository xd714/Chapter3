
## Complete Guide: Theory, Methods, and Mathematical Foundation

---

## 1. Problem Setup

### 1.1 Pedigree Structure

```
Animal  Sire  Dam   Status
1       0     0     Base animal (founder)
2       0     0     Base animal (founder, inbred)
3       1     2     Offspring with data
4       1     2     Offspring with data (target animal)
5       1     2     Offspring with data
```

### 1.2 Data and Model Parameters

- **Observations:** y₃ = 17, y₄ = 4, y₅ = 21
- **Model:** y = μ + u + e
- **Variance components:** σ²ₑ = 6, σ²ᵤ = 2
- **Variance ratio:** λ = σ²ₑ/σ²ᵤ = 3

### 1.3 Research Question

**Calculate PEV(MS₄)** where MS₄ = u₄ - ½(u₁ + u₂)

_MS₄ represents the "new" genetic material in animal 4 beyond what it inherited from its parents_

---

## 2. Mathematical Foundation: Henderson's Mixed Model Theory

### 2.1 The Problem We're Solving

We have a **mixed linear model**:

```
y = Xβ + Zu + e
```

Where:

- **y** = observed data vector
- **β** = fixed effects (unknown parameters)
- **u** = random effects with **E[u] = 0**, **Var(u) = G**
- **e** = residual errors with **E[e] = 0**, **Var(e) = R**
- **u** and **e** are independent

We want to find **BLUP** (Best Linear Unbiased Predictor) solutions for both β and u.

### 2.2 Derivation Method 1: Joint Density Approach

#### Step 1: Joint Distribution

The joint density of **(y, u)** is:

```
f(y,u) ∝ exp[-½(y-Xβ-Zu)'R⁻¹(y-Xβ-Zu) - ½u'G⁻¹u]
```

#### Step 2: Maximize Joint Likelihood

To find BLUP solutions, we **maximize this joint density** with respect to β and u.

Taking **partial derivatives** and setting equal to zero:

**∂/∂β:**

```
X'R⁻¹(y - Xβ - Zu) = 0
```

This gives us: **X'R⁻¹y = X'R⁻¹Xβ + X'R⁻¹Zu**

**∂/∂u:**

```
Z'R⁻¹(y - Xβ - Zu) - G⁻¹u = 0
```

This gives us: **Z'R⁻¹y = Z'R⁻¹Xβ + Z'R⁻¹Zu + G⁻¹u**

#### Step 3: Arrange in Matrix Form

```
[X'R⁻¹X    X'R⁻¹Z  ] [β̂]   [X'R⁻¹y]
[Z'R⁻¹X  Z'R⁻¹Z+G⁻¹] [û] = [Z'R⁻¹y]
```

### 2.3 Derivation Method 2: Minimizing Prediction Error

#### Step 1: What We Want to Minimize

For BLUP, we minimize the **prediction error variance** subject to **unbiasedness**.

The **prediction error** for any linear predictor **û = Ly** is:

```
E[(û - u)'(û - u)] = E[(Ly - u)'(Ly - u)]
```

#### Step 2: Unbiasedness Constraint

For unbiasedness: **E[û] = E[u] = 0**

Since **E[y] = Xβ**, we need:

```
E[Ly] = LE[y] = LXβ = 0  for all β
```

This requires: **LX = 0**

#### Step 3: Minimize Subject to Constraint

Using **Lagrange multipliers**, we minimize:

```
E[(Ly - u)'(Ly - u)] + 2λ'(LX)
```

This leads to the **normal equations** that produce exactly the same MME structure.

### 2.4 Derivation Method 3: Generalized Least Squares Perspective

#### Step 1: Extended Model Setup

Consider the **augmented system**:

```
[y] = [X  Z] [β] + [e]
[0]   [0  I] [u]   [-u]
```

With covariance structure:

```
Var([y]) = [R      0 ]
   [0]     [0  G⁻¹⁻¹]
```

#### Step 2: Generalized Least Squares

Applying **GLS** to this augmented system gives:

```
([X  Z]' [R⁻¹  0   ] [X  Z])⁻¹ [X  Z]' [R⁻¹  0   ] [y]
 [0  I]  [0    G⁻¹]  [0  I]      [0  I]  [0    G⁻¹]  [0]
```

Working out the matrix multiplications:

```
[X'R⁻¹X    X'R⁻¹Z  ] [β̂]   [X'R⁻¹y]
[Z'R⁻¹X  Z'R⁻¹Z+G⁻¹] [û] = [Z'R⁻¹y]
```

### 2.5 Why Each Component Appears

#### **X'R⁻¹X** (Top-left block)

- Represents **information about fixed effects** from the data
- **R⁻¹** weights observations by their **precision** (inverse variance)
- Standard **weighted least squares** form for fixed effects

#### **X'R⁻¹Z** and **Z'R⁻¹X** (Off-diagonal blocks)

- Capture **coupling between fixed and random effects**
- Both β and u affect the same observations y
- **Cannot estimate them independently**

#### **Z'R⁻¹Z + G⁻¹** (Bottom-right block)

- **Z'R⁻¹Z**: Information about random effects from the **data**
- **G⁻¹**: **Prior information** about random effects (shrinkage)
- **Addition** represents combining **data information** with **prior knowledge**

#### **X'R⁻¹y** and **Z'R⁻¹y** (Right-hand side)

- **Weighted projections** of data onto design matrices
- **R⁻¹** ensures proper weighting by observation precision

### 2.6 The Deep Mathematical Logic

The MME structure emerges because we're solving a **constrained optimization problem**:

1. **Minimize prediction error** (accuracy)
2. **Subject to unbiasedness** (no systematic bias)
3. **Accounting for covariance structure** (R and G)
4. **Jointly estimating** fixed and random effects

The **block matrix structure** naturally arises from:

- **Joint estimation** creates off-diagonal coupling terms
- **Different treatments** of β (fixed) vs u (random) create different diagonal blocks
- **Covariance weighting** (R⁻¹, G⁻¹) ensures optimal use of information

This is why Henderson's MME is not just a computational trick - it's the **mathematically optimal solution** to the mixed model prediction problem, emerging naturally from multiple theoretical approaches (maximum likelihood, minimum variance, generalized least squares).

### 2.7 Henderson's Transformation Steps

#### Step 1: Starting from BLUP Theory

Henderson's MME begins with the **Best Linear Unbiased Prediction (BLUP)** framework. The key insight is that BLUP solutions minimize prediction error variance subject to unbiasedness constraints.

**Why the original form is correct:**

```
[X'R⁻¹X    X'R⁻¹Z  ] [β̂]   [X'R⁻¹y]
[Z'R⁻¹X  Z'R⁻¹Z+G⁻¹] [û] = [Z'R⁻¹y]
```

This comes from **jointly solving** two optimization problems:

- Minimize prediction error variance for random effects **û**
- Ensure unbiased estimation of fixed effects **β̂**

The matrix structure emerges from the **normal equations** of this constrained optimization.

#### Step 2: Substituting Variance Components

**Why the substitution R = Iσ²ₑ and G = Aσ²ᵤ is correct:**

This assumes:

- **Residual errors are independent:** R = Iσ²ₑ (diagonal covariance)
- **Random effects follow genetic model:** G = Aσ²ᵤ (A is relationship matrix)

Substituting gives:

```
[X'X/σ²ₑ       X'Z/σ²ₑ     ] [β̂]   [X'y/σ²ₑ]
[Z'X/σ²ₑ  Z'Z/σ²ₑ+A⁻¹/σ²ᵤ] [û] = [Z'y/σ²ₑ]
```

**This is algebraically correct** because:

- (Iσ²ₑ)⁻¹ = I/σ²ₑ
- (Aσ²ᵤ)⁻¹ = A⁻¹/σ²ᵤ

#### Step 3: Scaling by σ²ₑ

**Why multiplying through by σ²ₑ is valid:**

This is a **consistent scaling operation** - multiplying both sides of a matrix equation by the same scalar preserves the solution:

```
[X'X      X'Z    ] [β̂]   [X'y]
[Z'X  Z'Z+A⁻¹λ] [û] = [Z'y]
```

where **λ = σ²ₑ/σ²ᵤ** is the **variance ratio**.

**Why λ makes sense:** It represents the relative importance of residual vs. genetic variance. Higher λ means more shrinkage toward zero for random effects.

#### Step 4: Henderson's Fundamental Variance Result

**Why PEV = σ²ₑ × C⁻¹ is mathematically correct:**

This is Henderson's most profound insight. Here's the step-by-step proof:

##### Starting Point: Error Structure

For the MME solution:

```
[β̂] = C⁻¹ [X'y]
[û]       [Z'y]
```

The **prediction errors** are:

```
[β̂ - β] = C⁻¹ [X'y - X'Xβ - X'Zu]
[û - u]       [Z'y - Z'Xβ - Z'Zu]
```

Since **y = Xβ + Zu + e**, we have:

- X'y - X'Xβ - X'Zu = X'e
- Z'y - Z'Xβ - Z'Zu = Z'e

Therefore:

```
[β̂ - β] = C⁻¹ [X'e]
[û - u]       [Z'e]
```

##### Taking the Variance

```
Var([β̂ - β]) = Var(C⁻¹ [X'e]) = C⁻¹ Var([X'e]) C⁻¹'
   [û - u]           [Z'e]            [Z'e]
```

##### Key Calculation

Since **Var(e) = Iσ²ₑ**:

```
Var([X'e]) = [X' 0] Var(e) [X' 0]' = [X'X  X'Z] σ²ₑ
   [Z'e]     [0  Z']        [0  Z']   [Z'X  Z'Z]
```

##### Henderson's Crucial Insight

The **structure of MME** creates a special relationship:

```
C⁻¹ [X'X  X'Z] C⁻¹' = C⁻¹
    [Z'X  Z'Z]
```

This happens because **C** itself contains these same **X'X, X'Z, Z'X, Z'Z** terms, and the inverse operation creates this elegant simplification.

**Therefore:**

```
Var([β̂ - β]) = σ²ₑ × C⁻¹
   [û - u]
```

#### Step 5: Why This Makes Intuitive Sense

**The σ²ₑ factor:**

- **All uncertainty originates from residual error** - if σ²ₑ = 0, predictions would be perfect
- **Scales the entire uncertainty structure** - double the residual variance, double all prediction errors

**The C⁻¹ structure:**

- **C represents available information** - larger elements mean more information
- **C⁻¹ inverts this relationship** - more information leads to smaller prediction errors
- **Captures correlations** - off-diagonal elements of C⁻¹ show how estimation errors are correlated

#### Mathematical Elegance

Henderson's result is remarkable because it shows that **all the complexity** of mixed model prediction errors - accounting for genetic relationships, variance components, and joint estimation - **reduces to one simple formula**:

**PEV = σ²ₑ × C⁻¹**

This makes computing prediction error variances computationally tractable and provides deep insight into the uncertainty structure of mixed model predictions.

#### Numerical Verification

**Let's verify with our example:**

**From our coefficient matrix C, we computed:**

```
C⁻¹ = [0.6555  -0.2680  -0.3349  -0.3408  -0.3626  -0.2630]
      [-0.2680   0.3182   0.3144   0.3109   0.3160   0.1769]
      ...
```

**Multiplying by σ²ₑ = 6:**

```
LHSI = 6 × C⁻¹ = [3.933  -1.608  -2.009  -2.045  -2.175  -1.578]
                  [-1.608   1.909   1.886   1.865   1.896   1.062]
                  ...
```

**This gives PEV(û₄) = 2.839, which means:**

- Standard error = √2.839 = 1.685
- The uncertainty in predicting u₄ depends on both the genetic relationships (A⁻¹) and the residual noise (σ²ₑ)

---

## 3. Method 1: Complete MME Approach (Henderson's Rigorous Method)

_This is Henderson's main theoretical contribution - always correct but computationally intensive_

### 3.1 Build Additive Relationship Matrix A

```
A = [1.000  1.000  1.000  1.000  0.500]
    [1.000  1.500  1.250  1.250  0.625]  ← Dam 2 is inbred (A₂₂ = 1.5)
    [1.000  1.250  1.500  1.125  0.563]
    [1.000  1.250  1.125  1.500  0.750]
    [0.500  0.625  0.563  0.750  1.000]
```

### 3.2 Calculate A⁻¹

```
A⁻¹ = [4.333  -0.667  -1.333  -1.333   0.000]
      [-0.667   3.333  -1.333  -1.333   0.000]
      [-1.333  -1.333   2.667   0.000   0.000]
      [-1.333  -1.333   0.000   3.067  -0.800]
      [0.000   0.000   0.000  -0.800   1.600]
```

### 3.3 Construct MME System

**Design matrices:**

```
X = [1]  (intercept)     Z = [0  0  1  0  0]  (animal incidence)
    [1]                      [0  0  0  1  0]
    [1]                      [0  0  0  0  1]

y = [17]
    [4]
    [21]
```

**Coefficient matrix C:**

```
C = [X'X    X'Z   ]  =  [3    0    0    1    1    1  ]
    [Z'X  Z'Z+A⁻¹λ]     [0   13   -2   -4   -4    0  ]
                        [0   -2   10   -4   -4    0  ]
                        [1   -4   -4    9    0    0  ]
                        [1   -4   -4    0   10.2 -2.4]
                        [1    0    0    0   -2.4  4.8]
```

**Right-hand side:**

```
RHS = [X'y] = [42]
      [Z'y]   [0]
              [0]
              [17]
              [4]
              [21]
```

### 3.4 Solve for BLUP Solutions

```
[μ̂ ]   [14.762]  ← Overall mean
[û₁] = [-0.988]  ← Breeding value of sire
[û₂]   [-1.235]  ← Breeding value of dam (inbred)
[û₃]   [-0.740]  ← Breeding value of offspring 3
[û₄]   [-1.855]  ← Breeding value of offspring 4 (target)
[û₅]   [0.308 ]  ← Breeding value of offspring 5
```

### 3.5 Calculate PEV Matrix Using Henderson's Formula

**C⁻¹ (first, before multiplying by σ²ₑ):**

```
C⁻¹ = [0.6555  -0.2680  -0.3349  -0.3408  -0.3626  -0.2630]
      [-0.2680   0.3182   0.3144   0.3109   0.3160   0.1769]
      [-0.3349   0.3144   0.4763   0.3886   0.3950   0.2212]
      [-0.3408   0.3109   0.3886   0.4598   0.3562   0.2061]
      [-0.3626   0.3160   0.3950   0.3562   0.4732   0.2583]
      [-0.2630   0.1769   0.2212   0.2061   0.2583   0.3247]
```

**PEV Matrix = LHSI = σ²ₑ × C⁻¹ = 6 × C⁻¹:**

```
LHSI = [3.933  -1.608  -2.009  -2.045  -2.175  -1.578]
       [-1.608   1.909   1.886   1.865   1.896   1.062]
       [-2.009   1.886   2.858   2.332   2.370   1.327]
       [-2.045   1.865   2.332   2.759   2.137   1.237]
       [-2.175   1.896   2.370   2.137   2.839   1.550]
       [-1.578   1.062   1.327   1.237   1.550   1.948]
```

### 3.6 Apply Contrast Vector for MS₄

**For MS₄ = u₄ - ½(u₁ + u₂):**

Contrast vector: K = [0, -0.5, -0.5, 0, 1, 0]

```
PEV(MS₄) = K × LHSI × K'= 0.708
```

**Result:** PEV(MS₄) = **0.708**

---

## 4. Method 2: Convenient Decomposition Method

_Uses Henderson's MME results but avoids complex contrast vector calculations_

### 4.1 Theoretical Foundation

**Key insight:** Every breeding value decomposes as:

```
u₄ = PA₄ + MS₄
```

where PA₄ (Parent Average) and MS₄ (Mendelian Sampling) are **independent**.

**Therefore:**

```
PEV(u₄) = PEV(PA₄) + PEV(MS₄)
```

### 4.2 Extract Values from LHSI Matrix

From Method 1 results:

- PEV(u₁) = LHSI[2,2] = 1.909
- PEV(u₂) = LHSI[3,3] = 2.858
- PEV(u₄) = LHSI[5,5] = 2.839
- Cov(u₁, u₂) = LHSI[2,3] = 1.886

### 4.3 Calculate PEV(PA₄)

**Parent Average:** PA₄ = ½(u₁ + u₂)

```
PEV(PA₄) = Var[½(û₁ + û₂)]
         = ¼[Var(û₁) + Var(û₂) + 2Cov(û₁,û₂)]
         = ¼[1.909 + 2.858 + 2×1.886]
         = ¼[8.539]
         = 2.135
```

### 4.4 Calculate PEV(MS₄)

```
PEV(MS₄) = PEV(u₄) - PEV(PA₄)
         = 2.839 - 2.135
         = 0.704
```

**Result:** PEV(MS₄) = **0.704** _(small difference due to rounding)_

---

## 5. Method 3: Henderson's Approximation Formula

_Henderson's quick approximation - works for simple cases but fails with complex pedigrees_

### 5.1 Henderson's Approximation Formula

For offspring of sire s and dam d:

```
Var(MS) = σ²ᵤ × [1 - ¼(A_ss + A_dd + 2A_sd)]
```

**Important:** This is a **simplified approximation** derived for standard, outbred populations.

### 5.2 Extract Relationship Coefficients

For animal 4 with parents 1 and 2:

- A₁₁ = 1.000 (sire relationship with himself)
- A₂₂ = 1.500 (dam relationship with herself - **inbred!**)
- A₁₂ = 1.000 (sire-dam relationship)

### 5.3 Apply Formula - Why It Fails

```
Var(MS₄) = σ²ᵤ × [1 - ¼(A₁₁ + A₂₂ + 2A₁₂)]
         = 2 × [1 - ¼(1.000 + 1.500 + 2×1.000)]
         = 2 × [1 - ¼(5.500)]
         = 2 × [1 - 1.375]
         = 2 × (-0.375)
         = -0.750
```

**Problem:** **Negative variance is mathematically impossible!**

### 5.4 Why This Method Fails Here

1. **Inbred dam:** A₂₂ = 1.5 > 1.0 violates the formula's assumptions
2. **Complex relationships:** The simple formula can't handle this genetic structure
3. **Formula limitations:** Designed only for standard outbred populations

**The sum A₁₁ + A₂₂ + 2A₁₂ = 5.5 > 4.0** breaks the assumption that this should be ≤ 4.0 for the formula to work.

### 5.5 Alternative Approach for Comparison

For typical half-sibs from unrelated parents:

```
Standard Var(MS) = 0.5 × σ²ᵤ = 0.5 × 2 = 1.0
```

Adjusting for prediction accuracy due to having data:

```
PEV(MS₄) ≈ 1.0 - 0.292 = 0.708
```

_(The "accuracy effect" of 0.292 represents uncertainty reduction from 1.0 to 0.708 due to information from data)_

---

## 6. Results Summary and Comparison

### 6.1 The Hierarchy of Methods

1. **Method 1 (Complete MME)** - Henderson's **theoretical foundation**
    
    - The rigorous, complete solution
    - Always correct but computationally expensive
2. **Method 2 (Convenient)** - **Practical application** of Henderson's theory
    
    - Uses MME results with efficient computation
    - Best balance for real-world use
3. **Method 3 (Approximation)** - Henderson's **simplified formula**
    
    - Quick shortcut for simple cases only
    - Fails when assumptions are violated

### 6.2 Results Comparison

|Method|Approach|PEV(MS₄)|Computational Cost|Reliability|
|---|---|---|---|---|
|**Method 1**|Complete MME|**0.708**|High O(n³)|Always correct|
|**Method 2**|Convenient decomposition|**0.704**|Low O(1)|Very reliable|
|**Method 3**|Henderson's approximation|**Fails (-0.750)**|Very low O(1)|Limited scope|

### 6.3 Key Formulas Summary

**Method 1 (Complete MME):**

```
PEV(MS₄) = K × (σ²ₑ × C⁻¹) × K'
where K = [0, -0.5, -0.5, 0, 1, 0] (contrast vector)
```

**Method 2 (Convenient Decomposition):**

```
PEV(MS₄) = PEV(u₄) - PEV(PA₄)
where PEV(PA₄) = ¼[PEV(u₁) + PEV(u₂) + 2Cov(u₁,u₂)]
```

**Method 3 (Approximation - when applicable):**

```
Var(MS₄) = σ²ᵤ × [1 - ¼(A_ss + A_dd + 2A_sd)]
(Fails when A_ss + A_dd + 2A_sd > 4.0)
```

---

## 7. Practical Recommendations

### 7.1 When to Use Each Method

1. **Use Method 1** when:
    
    - Maximum accuracy is required
    - Computational resources are available
    - Complex pedigree structures present
    - Need full covariance matrix
2. **Use Method 2** when:
    
    - Good balance of accuracy and efficiency needed
    - MME has already been solved
    - Most practical applications
    - Quick MS calculations required
3. **Avoid Method 3** when:
    
    - Any inbreeding is present
    - Complex genetic structures exist
    - High accuracy is required
    - A_ss + A_dd + 2A_sd > 4.0

### 7.2 Biological Interpretation

**PEV(MS₄) = 0.708** means:

- **Standard error** = √0.708 = 0.841
- **Biological meaning:** Uncertainty in the "new" genetic contribution from animal 4
- **95% confidence interval:** MS₄ ± 1.65 (approximately):
    1. **95% confidence** → **Critical value = 1.96** (from normal distribution)
    2. **Margin of error = 1.96 × 0.841 = 1.65**
    3. **Therefore: 95% CI = MS₄ ± 1.65**
- **Information content:** Data provides substantial information, reducing uncertainty from theoretical maximum (~1.0) to 0.708

---