# GigaMUGA mm39 Conversion Pipeline - Code Explanation

## Why We Need This Pipeline

**The Problem**: Our GigaMUGA mouse genotype data uses mm9 coordinates (old mouse genome), but modern analysis tools require mm39 coordinates (current mouse genome). We can't just use the old coordinates because:

1. **Different positions**: The same SNP is at position 12,345,678 in mm9 but 12,567,890 in mm39
2. **Assembly improvements**: mm39 has better accuracy and corrected errors from mm9
3. **Tool compatibility**: Current GWAS and analysis tools expect mm39 coordinates
4. **Database integration**: All modern mouse databases use mm39

**Our Solution**: A streamlined integrated pipeline that safely converts coordinates while preserving data integrity through a single comprehensive script.

---

## Unified Pipeline Architecture

Unlike the previous 4-step approach, the refined pipeline integrates all operations into a single cohesive workflow that mirrors the R script implementation:

### **Core Files:**

- **`gigamuga_pipeline.py`**: Main Python script (integrated approach)
- **`liftover_coords.sh`**: Specialized liftOver wrapper
- **`run_pipeline.sh`**: Complete pipeline orchestrator

### **Key Architectural Improvements:**

**1. Integrated Workflow** - All coordinate conversion, dataset updating, and quality control in one script **2. Modular Execution** - Can run in `prepare`, `complete`, or `auto` modes **3. Robust Error Handling** - Comprehensive validation at each step **4. Real-time Progress** - Detailed feedback during long operations

---

## Pipeline Flow and Operations

### **Step 1: Coordinate Preparation and Array Processing**

**What This Does**: Reads the GigaMUGA array definition and prepares coordinates for conversion.

```python
def prepare_coordinates():
    # Read and validate array file
    array = pd.read_csv('Marker_Giga_MUGA.csv')
    array = array.dropna(subset=['Name', 'Chr', 'MapInfo'])
    array = array[array['MapInfo'] > 0]
    
    # Create BED file for liftOver (0-based coordinates)
    bed_mm9 = pd.DataFrame({
        'chr': 'chr' + array['Chr'].astype(str),
        'start': array['MapInfo'] - 1,  # Convert to 0-based
        'end': array['MapInfo'],
        'name': array['Name']
    })
    
    bed_mm9.to_csv('temp_mm9.bed', sep='\t', header=False, index=False)
```

**Key Improvements from R Script:**

- **Conservative filtering**: Only processes markers with valid coordinates
- **Proper coordinate conversion**: Handles 1-based to 0-based conversion correctly
- **Validation at input**: Ensures data quality before processing

### **Step 2: LiftOver Coordinate Conversion**

**What This Does**: Converts mm9 coordinates to mm39 using UCSC liftOver with detailed tracking.

```bash
# Two-step conversion for maximum accuracy
echo "Step 1: Converting mm9 → mm10..."
liftOver temp_mm9.bed mm9ToMm10.over.chain.gz temp_mm10.bed temp_fail1.bed

echo "Step 2: Converting mm10 → mm39..."
liftOver temp_mm10.bed mm10ToMm39.over.chain.gz temp_mm39.bed temp_fail2.bed

# Calculate comprehensive statistics
overall_success_rate=$(echo "scale=1; $mm39_count * 100 / $original_count" | bc -l)
```

**Why Two Steps**: Following the R script approach, we use mm9→mm10→mm39 rather than direct conversion for better accuracy and to track failures at each step.

### **Step 3: Dataset Integration and Coordinate Update**

**What This Does**: Applies converted coordinates to the actual dataset with comprehensive validation.

```python
def complete_pipeline():
    # Read converted coordinates
    coords_mm39 = pd.read_csv('temp_mm39.bed', sep='\t', header=None, 
                             names=['chr', 'start', 'end', 'name'])
    coords_mm39['pos'] = coords_mm39['start'] + 1  # Convert back to 1-based
    
    # Read failed conversions for exclusion
    failed_snps = set()
    for fail_file in ['temp_fail1.bed', 'temp_fail2.bed']:
        if os.path.exists(fail_file) and os.path.getsize(fail_file) > 0:
            # Extract failed SNP names
            
    # Filter dataset conservatively
    bim = pd.read_csv('DU6_inter_1.bim', sep='\t', header=None,
                     names=['chr', 'snp', 'dist', 'pos', 'a1', 'a2'])
    
    converted_snps = set(coords_mm39['name'])
    keep_snps = bim[bim['snp'].isin(converted_snps) & ~bim['snp'].isin(failed_snps)]
```

**Key Logic from R Script:**

- **Conservative approach**: Only update SNPs we're absolutely confident about
- **Failed SNP exclusion**: Remove any SNPs that failed conversion at either step
- **Chromosome validation**: Check for chromosome mismatches after conversion

### **Step 4: Reference Alignment and Strand Checking**

**What This Does**: Aligns alleles with the mm39 reference genome for consistency.

```python
# Reference alignment logic (following R script approach)
for i, row in bim_final.iterrows():
    # Query reference genome
    cmd = f"samtools faidx mm39.fa {row['chr']}:{row['pos']}-{row['pos']}"
    success, result = run_command(cmd, silent=True)
    
    ref_allele = lines[1].strip().upper()
    
    # Check if strand flip needed (R script logic)
    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}
    if ref_allele == row['a2'] or ref_allele == complement.get(row['a1'], ''):
        flip_snps.append(row['snp'])
```

**Reference Alignment Strategy** (following R implementation):

- **Systematic checking**: Compare each SNP against reference
- **Complement logic**: Handle both strand orientations
- **Conservative exclusion**: Remove SNPs that don't align properly

### **Step 5: Quality Control and Final Validation**

**What This Does**: Removes monomorphic SNPs and performs final quality assessment.

```python
# Quality validation following R script
success, _ = run_command("plink1 --bfile temp_final --freq --out temp_validation --silent")
freq_data = pd.read_csv('temp_validation.frq', sep=r'\s+')
monomorphic = ((freq_data['MAF'] == 0) | (freq_data['MAF'] == 0.5)).sum()

if monomorphic > 0:
    print(f"⚠ Removing {monomorphic} monomorphic SNPs")
    success, _ = run_command("plink1 --bfile temp_final --maf 0.01 --make-bed --out final_dataset_polymorphic --silent")
```

---

## Key Architectural Refinements from R Script

### **1. Simplified File Management**

**Before (4-step)**: Multiple intermediate files across steps

```
step1_array_mm39_coordinates.bed
step2_DU6_mm39.bed/bim/fam  
step3_DU6_final.bed/bim/fam
step4_validation_report.txt
```

**After (integrated)**: Clean temporary file handling

```
temp_mm9.bed → temp_mm10.bed → temp_mm39.bed
temp_filtered.bed/bim/fam → temp_final.bed/bim/fam
final_dataset_polymorphic.bed/bim/fam
```

### **2. Streamlined Error Handling**

**R Script Approach**: Immediate validation and conservative decisions

```r
# Check for chromosome mismatches
any(bim_updated$chr_orig != bim_updated$chr_new) 

# Conservative SNP retention
keep_snps <- bim$snp[bim$snp %in% coords_mm39$name & !bim$snp %in% failed_snps]
```

**Implementation**: Integrated validation throughout pipeline

```python
# Check for chromosome mismatches
chr_mismatches = (bim_updated['chr'].astype(str) != bim_updated['chr_new'].astype(str)).sum()
if chr_mismatches > 0:
    print(f"⚠ Warning: {chr_mismatches} SNPs mapped to different chromosomes - removing them")
    bim_updated = bim_updated[bim_updated['chr'].astype(str) == bim_updated['chr_new'].astype(str)]
```

### **3. Enhanced Progress Reporting**

**R Script Pattern**: Detailed progress with meaningful checkpoints

```r
cat("✓ Converted", nrow(coords_mm39), "SNPs,", length(failed_snps), "failed\n")
cat("✓ Reference alignment complete - Success rate:", success_rate, "%\n")
cat("✓ Applied", length(flip_snps), "strand flips\n")
```

**Implementation**: Real-time feedback during operations

```python
print(f"✓ Successfully converted {len(coords_mm39)} positions to mm39")
print(f"✓ Success rate: {success_rate}%")
print(f"✓ SNPs flagged for strand flipping: {len(flip_snps)}")
```

### **4. Comprehensive Quality Assessment**

**R Script Logic**: Multi-dimensional quality evaluation

```r
if(retention > 85) cat("✓ Excellent retention!\n")
elif(retention > 75) cat("✓ Good retention")  
else cat("⚠ Low retention - review recommended")
```

**Implementation**: Integrated quality metrics

```python
if retention > 85:
    report_lines.append("Status: EXCELLENT retention rate")
elif retention > 75:
    report_lines.append("Status: GOOD retention rate")
else:
    report_lines.append("Status: LOW retention rate - review recommended")
```

---

## Pipeline Execution Modes

### **Mode 1: Automatic (Default)**

```bash
python3 gigamuga_pipeline.py
# Automatically determines what step to run based on existing files
```

### **Mode 2: Prepare Only**

```bash
python3 gigamuga_pipeline.py prepare
# Creates temp_mm9.bed and stops for manual liftOver
```

### **Mode 3: Complete After LiftOver**

```bash
python3 gigamuga_pipeline.py complete  
# Assumes liftOver files exist, completes pipeline
```

### **Mode 4: Full Orchestrated Pipeline**

```bash
bash run_pipeline.sh
# Handles entire pipeline including liftOver step
```

---

## Output and Results

### **Final Dataset Files:**

- **`final_dataset_polymorphic.bed/bim/fam`**: Complete dataset with mm39 coordinates
- **`pipeline_report.txt`**: Comprehensive quality and conversion statistics

### **Quality Metrics Provided:**

- **Retention rate**: Percentage of original SNPs retained
- **Conversion success**: liftOver success rates for each step
- **Reference alignment**: Percentage successfully aligned to mm39 reference
- **Strand corrections**: Number of allele flips applied
- **Monomorphic removal**: Count of invariant SNPs removed

### **Quality Assessment Categories:**

- **EXCELLENT**: >85% retention with good reference alignment
- **GOOD**: >75% retention with acceptable metrics
- **MODERATE**: <75% retention or significant quality issues

---

## Why This Refined Approach is Better

### **1. Reduced Complexity**

- **Single main script** instead of 4 separate steps
- **Integrated validation** throughout pipeline
- **Cleaner file management** with automatic cleanup

### **2. Improved Reliability**

- **Conservative filtering** at every step
- **Comprehensive error handling** following R script patterns
- **Real-time progress feedback** for long operations

### **3. Better User Experience**

- **Multiple execution modes** for different use cases
- **Clear quality assessment** with actionable feedback
- **Comprehensive reporting** for reproducibility

### **4. Enhanced Maintainability**

- **Modular functions** that can be tested independently
- **Consistent coding patterns** across Python and shell components
- **Clear documentation** of every transformation

This refined pipeline maintains the robustness of the original multi-step approach while providing the simplicity and integration demonstrated in the R script implementation.