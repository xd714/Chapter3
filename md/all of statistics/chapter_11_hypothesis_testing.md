# Chapter 11: Hypothesis Testing and p-values - Mathematical Explanations

## Overview
Hypothesis testing provides a framework for making decisions about population parameters based on sample data. This chapter covers the fundamental concepts of null and alternative hypotheses, test statistics, p-values, and various testing procedures.

## 11.1 Introduction to Hypothesis Testing

### Basic Framework
We partition the parameter space Œò into two disjoint sets Œò‚ÇÄ and Œò‚ÇÅ and wish to test:
```
H‚ÇÄ: Œ∏ ‚àà Œò‚ÇÄ  (null hypothesis)
versus
H‚ÇÅ: Œ∏ ‚àà Œò‚ÇÅ  (alternative hypothesis)
```

### Decision Rule
Let X be a random variable with range ùí≥. We test a hypothesis by finding an appropriate subset R ‚äÜ ùí≥ called the **rejection region**:
```
X ‚àà R    ‚üπ reject H‚ÇÄ
X ‚àâ R    ‚üπ retain (do not reject) H‚ÇÄ
```

Usually the rejection region has the form:
```
R = {x : T(x) > c}
```
where T is a **test statistic** and c is a **critical value**.

## 11.2 Types of Errors

### Type I and Type II Errors
|               | H‚ÇÄ true    | H‚ÇÄ false   |
|---------------|------------|------------|
| Reject H‚ÇÄ     | Type I     | Correct    |
| Retain H‚ÇÄ     | Correct    | Type II    |

### Error Probabilities
- **Type I Error (Œ±):** P(reject H‚ÇÄ | H‚ÇÄ true) = P(Type I error)
- **Type II Error (Œ≤):** P(retain H‚ÇÄ | H‚ÇÄ false) = P(Type II error)
- **Power:** 1 - Œ≤ = P(reject H‚ÇÄ | H‚ÇÄ false)

### Size and Level
- **Size of test:** Œ± = sup_{Œ∏‚ààŒò‚ÇÄ} P_Œ∏(X ‚àà R)
- **Level-Œ± test:** A test with size ‚â§ Œ±

## 11.3 Test Statistics and p-values

### Wald Test
For parameter Œ∏ with estimate Œ∏ÃÇ:
```
W = (Œ∏ÃÇ - Œ∏‚ÇÄ) / ≈ùe(Œ∏ÃÇ)
```

Under H‚ÇÄ: Œ∏ = Œ∏‚ÇÄ, W approximately follows N(0,1) for large samples.

**Rejection region:** |W| > z_{Œ±/2}

### Likelihood Ratio Test
```
Œª(x) = L(Œ∏ÃÇ‚ÇÄ)/L(Œ∏ÃÇ)
```

where:
- Œ∏ÃÇ‚ÇÄ = MLE under H‚ÇÄ
- Œ∏ÃÇ = unrestricted MLE

**Wilks' Theorem:** Under regularity conditions and H‚ÇÄ:
```
-2 ln Œª(X) ‚Üí œá¬≤_k
```
where k = dim(Œò) - dim(Œò‚ÇÄ).

### Score Test
Based on the score function at Œ∏‚ÇÄ:
```
S = ‚àÇ‚Ñì/‚àÇŒ∏|_{Œ∏=Œ∏‚ÇÄ}
```

**Test statistic:**
```
T = S¬≤/I(Œ∏‚ÇÄ)
```

where I(Œ∏‚ÇÄ) is the Fisher information.

Under H‚ÇÄ: T ‚Üí œá¬≤‚ÇÅ

## 11.4 p-values

### Definition
The **p-value** is the probability of observing a test statistic as extreme or more extreme than what was actually observed, assuming H‚ÇÄ is true:
```
p-value = sup_{Œ∏‚ààŒò‚ÇÄ} P_Œ∏(T(X) ‚â• T(x))
```

### Properties
1. If H‚ÇÄ is true, p-value ~ Uniform(0,1)
2. Smaller p-values indicate stronger evidence against H‚ÇÄ
3. **p-value ‚â† P(H‚ÇÄ|data)**

### Evidence Scale
| p-value    | Evidence                           |
|------------|----------------------------------- |
| < 0.01     | Very strong evidence against H‚ÇÄ    |
| 0.01-0.05  | Strong evidence against H‚ÇÄ         |
| 0.05-0.10  | Weak evidence against H‚ÇÄ           |
| > 0.10     | Little or no evidence against H‚ÇÄ   |

### Common p-value Calculations

**Two-sided test with normal statistic:**
```
p-value = 2P(|Z| > |z|) = 2Œ¶(-|z|)
```

**One-sided test:**
```
p-value = P(Z > z) = 1 - Œ¶(z)  [upper tail]
p-value = P(Z < z) = Œ¶(z)      [lower tail]
```

## 11.5 Specific Tests

### One-Sample t-test
Testing H‚ÇÄ: Œº = Œº‚ÇÄ for normal population with unknown œÉ.

**Test statistic:**
```
T = (XÃÑ - Œº‚ÇÄ)/(S/‚àön)
```

Under H‚ÇÄ: T ~ t_{n-1}

**p-value:**
- Two-sided: p = 2P(t_{n-1} > |t|)
- One-sided: p = P(t_{n-1} > t) or P(t_{n-1} < t)

### Two-Sample t-test
Testing H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ for two normal populations.

**Equal variances assumed:**
```
T = (XÃÑ‚ÇÅ - XÃÑ‚ÇÇ)/(S_p‚àö(1/n‚ÇÅ + 1/n‚ÇÇ))
```

where S¬≤_p = [(n‚ÇÅ-1)S¬≤‚ÇÅ + (n‚ÇÇ-1)S¬≤‚ÇÇ]/(n‚ÇÅ+n‚ÇÇ-2)

Under H‚ÇÄ: T ~ t_{n‚ÇÅ+n‚ÇÇ-2}

**Welch's t-test (unequal variances):**
```
T = (XÃÑ‚ÇÅ - XÃÑ‚ÇÇ)/‚àö(S¬≤‚ÇÅ/n‚ÇÅ + S¬≤‚ÇÇ/n‚ÇÇ)
```

Degrees of freedom approximated by Welch-Satterthwaite formula.

### Chi-square Goodness-of-fit Test
Testing H‚ÇÄ: distribution follows specified form.

**Pearson's œá¬≤ statistic:**
```
T = Œ£ (O_i - E_i)¬≤/E_i
```

where O_i = observed frequency, E_i = expected frequency.

Under H‚ÇÄ: T ‚Üí œá¬≤_{k-1-p} where p = number of estimated parameters.

### Chi-square Test of Independence
Testing independence in contingency tables.

**Test statistic:**
```
T = Œ£·µ¢ Œ£‚±º (O_{ij} - E_{ij})¬≤/E_{ij}
```

where E_{ij} = (row total √ó column total)/grand total.

Under H‚ÇÄ: T ‚Üí œá¬≤_{(r-1)(c-1)}

## 11.6 Multiple Testing

### Family-wise Error Rate (FWER)
Probability of making at least one Type I error:
```
FWER = P(reject at least one true H‚ÇÄ)
```

### Bonferroni Correction
For m tests, use significance level Œ±/m for each test:
```
P(at least one Type I error) ‚â§ m √ó (Œ±/m) = Œ±
```

**Adjusted p-values:** pÃÉ·µ¢ = min(1, m¬∑p·µ¢)

### Holm's Method
1. Order p-values: p‚Çç‚ÇÅ‚Çé ‚â§ ... ‚â§ p‚Çç‚Çò‚Çé
2. Find smallest k such that p‚Çç‚Çñ‚Çé > Œ±/(m-k+1)
3. Reject H‚Çç‚ÇÅ‚Çé, ..., H‚Çç‚Çñ‚Çã‚ÇÅ‚Çé

### False Discovery Rate (FDR)
Expected proportion of false discoveries among rejected hypotheses:
```
FDR = E[V/R | R > 0]
```

where V = false discoveries, R = total rejections.

**Benjamini-Hochberg procedure:**
1. Order p-values: p‚Çç‚ÇÅ‚Çé ‚â§ ... ‚â§ p‚Çç‚Çò‚Çé
2. Find largest k such that p‚Çç‚Çñ‚Çé ‚â§ (k/m)Œ±
3. Reject H‚Çç‚ÇÅ‚Çé, ..., H‚Çç‚Çñ‚Çé

## 11.7 Power and Sample Size

### Power Function
The **power function** is:
```
Œ≤(Œ∏) = P_Œ∏(X ‚àà R)
```

**Properties:**
- Œ≤(Œ∏) ‚â§ Œ± for Œ∏ ‚àà Œò‚ÇÄ (size constraint)
- Œ≤(Œ∏) should be large for Œ∏ ‚àà Œò‚ÇÅ (power requirement)

### Sample Size Calculation
For testing H‚ÇÄ: Œº = Œº‚ÇÄ vs H‚ÇÅ: Œº = Œº‚ÇÅ with significance level Œ± and power 1-Œ≤:
```
n = [œÉ¬≤(z_{Œ±/2} + z_Œ≤)¬≤] / (Œº‚ÇÅ - Œº‚ÇÄ)¬≤
```

**For one-sided test:**
```
n = [œÉ¬≤(z_Œ± + z_Œ≤)¬≤] / (Œº‚ÇÅ - Œº‚ÇÄ)¬≤
```

### Effect Size
**Cohen's d:**
```
d = (Œº‚ÇÅ - Œº‚ÇÄ) / œÉ
```

**Interpretation:**
- d = 0.2: small effect
- d = 0.5: medium effect  
- d = 0.8: large effect

## 11.8 Nonparametric Tests

### Sign Test
For median Œ∏, testing H‚ÇÄ: Œ∏ = Œ∏‚ÇÄ.

**Test statistic:** S = number of observations > Œ∏‚ÇÄ

Under H‚ÇÄ: S ~ Binomial(n, 1/2)

**p-value (two-sided):** p = 2min{P(S ‚â§ s), P(S ‚â• s)}

### Wilcoxon Signed-Rank Test
For symmetric distribution around Œ∏, testing H‚ÇÄ: Œ∏ = Œ∏‚ÇÄ.

**Procedure:**
1. Compute differences: D·µ¢ = X·µ¢ - Œ∏‚ÇÄ
2. Rank absolute differences: |D‚Çç‚ÇÅ‚Çé| ‚â§ ... ‚â§ |D‚Çç‚Çô‚Çé|
3. Test statistic: W‚Å∫ = sum of ranks for positive differences

**Distribution:** For large n, W‚Å∫ is approximately normal with:
- Mean: n(n+1)/4
- Variance: n(n+1)(2n+1)/24

### Mann-Whitney U Test
For testing H‚ÇÄ: F_X = F_Y (equal distributions).

**Test statistic:**
```
U = Œ£·µ¢ Œ£‚±º I(X·µ¢ > Y‚±º)
```

**Equivalent form:**
```
U = R_X - n_X(n_X + 1)/2
```

where R_X = sum of ranks for X sample.

**Distribution:** For large samples, U is approximately normal.

### Kolmogorov-Smirnov Test
Testing H‚ÇÄ: F = F‚ÇÄ (specified distribution).

**Test statistic:**
```
D_n = sup_x |FÃÇ_n(x) - F‚ÇÄ(x)|
```

**Critical values:** Depends on Kolmogorov distribution.

**Two-sample version:**
```
D_{m,n} = sup_x |FÃÇ_m(x) - ƒú_n(x)|
```

## 11.9 Permutation Tests

### Principle
If H‚ÇÄ implies exchangeability, all permutations of the data are equally likely under H‚ÇÄ.

### General Algorithm
1. Compute test statistic T_obs for observed data
2. Generate all (or many) permutations of the data
3. Compute test statistic for each permutation
4. p-value = proportion of permutation statistics ‚â• T_obs

### Two-Sample Permutation Test
Testing H‚ÇÄ: F_X = F_Y.

**Test statistic:** T = |XÃÑ - »≤|

**Permutation distribution:** All (m+n choose m) ways to assign labels to combined sample.

**p-value:**
```
p = (1/N!) Œ£ I(T_j > T_obs)
```

where sum is over all N! permutations.

## 11.10 Bootstrap Tests

### Bootstrap p-values
1. Compute test statistic T_obs
2. Generate bootstrap samples under H‚ÇÄ
3. Compute bootstrap test statistics T‚ÇÅ*, ..., T_B*
4. p-value = proportion of T_i* ‚â• T_obs

### Advantages
- Doesn't require asymptotic theory
- Adapts to actual sampling distribution
- Works for complex statistics

## 11.11 Bayesian Testing

### Bayes Factors
**Bayes factor** for H‚ÇÅ vs H‚ÇÄ:
```
BF‚ÇÅ‚ÇÄ = P(Data|H‚ÇÅ) / P(Data|H‚ÇÄ)
```

**Interpretation:**
- BF‚ÇÅ‚ÇÄ > 1: Evidence for H‚ÇÅ
- BF‚ÇÅ‚ÇÄ < 1: Evidence for H‚ÇÄ
- BF‚ÇÅ‚ÇÄ = 1: No evidence either way

### Posterior Odds
```
Posterior Odds = Prior Odds √ó Bayes Factor
```

```
P(H‚ÇÅ|Data) / P(H‚ÇÄ|Data) = [P(H‚ÇÅ)/P(H‚ÇÄ)] √ó BF‚ÇÅ‚ÇÄ
```

### Jeffreys' Scale for Bayes Factors
| BF‚ÇÅ‚ÇÄ        | Evidence for H‚ÇÅ                |
|-------------|--------------------------------|
| 1-3         | Barely worth mentioning        |
| 3-10        | Substantial                    |
| 10-30       | Strong                         |
| 30-100      | Very strong                    |
| >100        | Decisive                       |

## 11.12 Model Selection and Information Criteria

### Likelihood Ratio Tests for Nested Models
For nested models M‚ÇÄ ‚äÇ M‚ÇÅ:
```
LRT = -2[‚Ñì(Œ∏ÃÇ‚ÇÄ) - ‚Ñì(Œ∏ÃÇ‚ÇÅ)]
```

Under H‚ÇÄ (M‚ÇÄ is correct): LRT ‚Üí œá¬≤_k where k = difference in parameters.

### Akaike Information Criterion (AIC)
```
AIC = -2‚Ñì(Œ∏ÃÇ) + 2p
```

where p = number of parameters.

**Model selection:** Choose model with smallest AIC.

### Bayesian Information Criterion (BIC)
```
BIC = -2‚Ñì(Œ∏ÃÇ) + p ln(n)
```

**Properties:**
- BIC penalizes complexity more than AIC
- BIC is consistent for model selection
- AIC minimizes prediction error

## 11.13 Sequential Testing

### Sequential Probability Ratio Test (SPRT)
Testing H‚ÇÄ: Œ∏ = Œ∏‚ÇÄ vs H‚ÇÅ: Œ∏ = Œ∏‚ÇÅ.

**Likelihood ratio:**
```
Œõ_n = L(Œ∏‚ÇÅ)/L(Œ∏‚ÇÄ)
```

**Decision rule:**
- If Œõ_n ‚â• B: reject H‚ÇÄ
- If Œõ_n ‚â§ A: accept H‚ÇÄ  
- If A < Œõ_n < B: take another observation

**Boundaries:**
```
A = Œ≤/(1-Œ±)
B = (1-Œ≤)/Œ±
```

### Properties
- Minimizes expected sample size
- Maintains error probabilities Œ± and Œ≤
- Stopping time is finite with probability 1

## 11.14 Robustness and Assumptions

### Robustness of t-tests
- **Normality:** t-test robust to moderate deviations from normality
- **Equal variances:** Welch's t-test when variances unequal
- **Independence:** Violations can severely affect validity

### Diagnostic Methods
1. **Q-Q plots:** Check normality assumption
2. **Residual plots:** Check homoscedasticity
3. **Tests for assumptions:** Shapiro-Wilk, Levene's test

### Transformations
- **Log transformation:** For right-skewed data
- **Square root:** For count data
- **Box-Cox:** General power transformations

## 11.15 Modern Developments

### False Discovery Rate Control
Control expected proportion of false discoveries rather than family-wise error rate.

### High-dimensional Testing
- **Sparse signals:** Most null hypotheses true
- **Dependency:** Tests not independent
- **New methods:** Higher criticism, adaptive procedures

### Testing in Machine Learning
- **Cross-validation:** For model comparison
- **Permutation importance:** For feature selection
- **Stability selection:** For variable selection

## 11.16 Common Pitfalls and Misconceptions

### p-hacking and Multiple Comparisons
- **Cherry-picking:** Testing many hypotheses, reporting only significant ones
- **Data dredging:** Analyzing data until finding significance
- **HARKing:** Hypothesizing after results are known

### Misinterpretation of p-values
1. **p-value ‚â† P(H‚ÇÄ|data):** This is the prosecutor's fallacy
2. **Significance ‚â† practical importance:** Statistical vs practical significance
3. **Non-significance ‚â† no effect:** Absence of evidence ‚â† evidence of absence

### Base Rate Fallacy
Low base rate can make even accurate tests misleading:
```
PPV = (Sensitivity √ó Prevalence) / [Sensitivity √ó Prevalence + (1-Specificity) √ó (1-Prevalence)]
```

## 11.17 Practical Guidelines

### Choosing a Test
1. **Data type:** Continuous, discrete, categorical
2. **Sample size:** Large sample vs small sample methods
3. **Assumptions:** Parametric vs nonparametric
4. **Research question:** One-sided vs two-sided

### Reporting Results
1. **Effect size:** Not just p-value
2. **Confidence intervals:** Provide range of plausible values
3. **Assumptions:** State and check assumptions
4. **Multiple testing:** Adjust for multiple comparisons when appropriate

### Power Analysis
- **Prospective:** Determine sample size needed
- **Retrospective:** Assess power of completed study
- **Sensitivity analysis:** How results depend on assumptions

## 11.18 Connections to Other Chapters

### To Chapter 9 (Bootstrap)
- Bootstrap hypothesis tests
- Bootstrap p-values
- Permutation tests

### To Chapter 12 (Bayesian Inference)
- Bayesian vs frequentist testing
- Bayes factors vs p-values
- Posterior probabilities

### To Chapter 13 (Decision Theory)
- Optimal tests (Neyman-Pearson lemma)
- Minimax tests
- Risk functions

## Key Insights

1. **Testing Framework:** Hypothesis testing provides a systematic approach to decision-making under uncertainty.

2. **Error Control:** The trade-off between Type I and Type II errors is fundamental to test design.

3. **p-values:** Widely used but often misunderstood; they measure compatibility with null hypothesis, not evidence for it.

4. **Multiple Testing:** When testing many hypotheses, error rates must be carefully controlled.

5. **Assumptions Matter:** Violations of assumptions can invalidate test results.

## Important Formulas Summary

**Wald test:** W = (Œ∏ÃÇ - Œ∏‚ÇÄ)/≈ùe(Œ∏ÃÇ)

**Likelihood ratio:** Œª = L(Œ∏ÃÇ‚ÇÄ)/L(Œ∏ÃÇ)

**Chi-square statistic:** œá¬≤ = Œ£(O - E)¬≤/E

**Sample size (two-sided):** n = œÉ¬≤(z_{Œ±/2} + z_Œ≤)¬≤/(Œº‚ÇÅ - Œº‚ÇÄ)¬≤

**Bonferroni correction:** Œ±_adj = Œ±/m

**FDR procedure:** Reject if p‚Çç·µ¢‚Çé ‚â§ (i/m)Œ±

This chapter provides the foundation for making statistical decisions and forms the backbone of applied statistics across all fields.