# Chapter 10: Parametric Inference - Mathematical Explanations

## Overview
Parametric inference assumes the data comes from a distribution that belongs to a known family, characterized by a finite-dimensional parameter vector. This chapter covers maximum likelihood estimation, method of moments, and the asymptotic theory that underlies modern statistical inference.

## 10.1 Parametric Models

### Definition
A **parametric model** is a family of distributions:
```
ùí´ = {PŒ∏ : Œ∏ ‚àà Œò}
```

where Œò ‚äÜ ‚Ñù·µà is the parameter space.

### Examples

**Normal family:** ùí´ = {N(Œº, œÉ¬≤) : Œº ‚àà ‚Ñù, œÉ > 0}
- Parameter: Œ∏ = (Œº, œÉ¬≤)
- Parameter space: Œò = ‚Ñù √ó ‚Ñù‚Å∫

**Exponential family:** ùí´ = {Exp(Œª) : Œª > 0}
- Parameter: Œ∏ = Œª
- Parameter space: Œò = ‚Ñù‚Å∫

**Binomial family:** ùí´ = {Binomial(n, p) : p ‚àà [0,1]}
- Parameter: Œ∏ = p (n assumed known)
- Parameter space: Œò = [0,1]

### Identifiability
A model is **identifiable** if different parameters yield different distributions:
```
Œ∏ ‚â† Œ∏' ‚üπ PŒ∏ ‚â† PŒ∏'
```

**Non-identifiable example:** Mixture of two identical normals with unknown weights.

## 10.2 Method of Moments

### Population Moments
The **k-th population moment** is:
```
Œº‚Çñ(Œ∏) = E_Œ∏[X·µè]
```

### Sample Moments
The **k-th sample moment** is:
```
mÃÇ‚Çñ = (1/n) ‚àë·µ¢‚Çå‚ÇÅ‚Åø X·µ¢·µè
```

### Method of Moments Estimator
**Equate sample and population moments:**
```
mÃÇ‚Çñ = Œº‚Çñ(Œ∏), k = 1, ..., d
```

Solve this system for Œ∏ to get Œ∏ÃÇ‚Çò‚Çò.

### Examples

**Normal distribution N(Œº, œÉ¬≤):**
```
ŒºÃÇ = mÃÇ‚ÇÅ = XÃÑ
œÉÃÇ¬≤ = mÃÇ‚ÇÇ - (mÃÇ‚ÇÅ)¬≤ = (1/n)‚àëX·µ¢¬≤ - XÃÑ¬≤
```

**Gamma distribution Œì(Œ±, Œ≤):**
```
E[X] = Œ±Œ≤, Var(X) = Œ±Œ≤¬≤
Œ±ÃÇ = XÃÑ¬≤/S¬≤, Œ≤ÃÇ = S¬≤/XÃÑ
```

where S¬≤ is sample variance.

### Properties

**Consistency:** Under regularity conditions, Œ∏ÃÇ‚Çò‚Çò ‚Üí·µñ Œ∏

**Asymptotic normality:**
```
‚àön(Œ∏ÃÇ‚Çò‚Çò - Œ∏) ‚áù N(0, Œ£)
```

where Œ£ depends on moments and their derivatives.

**Efficiency:** Generally not efficient (doesn't achieve Cram√©r-Rao bound).

## 10.3 Maximum Likelihood Estimation

### Likelihood Function
For observations x‚ÇÅ, ..., x‚Çô, the **likelihood function** is:
```
L(Œ∏) = ‚àè·µ¢‚Çå‚ÇÅ‚Åø f(x·µ¢; Œ∏)
```

### Log-likelihood
```
‚Ñì(Œ∏) = log L(Œ∏) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø log f(x·µ¢; Œ∏)
```

### Maximum Likelihood Estimator (MLE)
```
Œ∏ÃÇ‚Çò‚Çó‚Çë = argmax_Œ∏ L(Œ∏) = argmax_Œ∏ ‚Ñì(Œ∏)
```

### First-order Conditions
**Score function:**
```
S(Œ∏) = ‚àá‚Ñì(Œ∏) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø ‚àá log f(x·µ¢; Œ∏)
```

**MLE satisfies:** S(Œ∏ÃÇ‚Çò‚Çó‚Çë) = 0 (if interior maximum)

### Examples

**Normal distribution (known œÉ¬≤):**
```
‚Ñì(Œº) = -n log(œÉ‚àö(2œÄ)) - (1/(2œÉ¬≤))‚àë(x·µ¢ - Œº)¬≤
‚àÇ‚Ñì/‚àÇŒº = (1/œÉ¬≤)‚àë(x·µ¢ - Œº) = 0
ŒºÃÇ = xÃÑ
```

**Exponential distribution:**
```
‚Ñì(Œª) = n log Œª - Œª‚àëx·µ¢
‚àÇ‚Ñì/‚àÇŒª = n/Œª - ‚àëx·µ¢ = 0
ŒªÃÇ = n/‚àëx·µ¢ = 1/xÃÑ
```

**Binomial distribution:**
```
‚Ñì(p) = ‚àëx·µ¢ log p + (n-‚àëx·µ¢) log(1-p)
‚àÇ‚Ñì/‚àÇp = ‚àëx·µ¢/p - (n-‚àëx·µ¢)/(1-p) = 0
pÃÇ = ‚àëx·µ¢/n = xÃÑ
```

## 10.4 Properties of Maximum Likelihood Estimators

### Consistency
Under regularity conditions:
```
Œ∏ÃÇ‚Çò‚Çó‚Çë ‚Üí·µñ Œ∏‚ÇÄ
```

where Œ∏‚ÇÄ is the true parameter.

### Asymptotic Normality
```
‚àön(Œ∏ÃÇ‚Çò‚Çó‚Çë - Œ∏‚ÇÄ) ‚áù N(0, I‚Åª¬π(Œ∏‚ÇÄ))
```

where I(Œ∏) is the Fisher information matrix.

### Asymptotic Efficiency
MLE achieves the Cram√©r-Rao lower bound asymptotically.

### Invariance Property
If Œ∏ÃÇ is MLE of Œ∏, then g(Œ∏ÃÇ) is MLE of g(Œ∏) for any function g.

### Equivariance
MLE is equivariant under reparameterization.

## 10.5 Fisher Information

### Definition
**Fisher information** measures the amount of information about parameter Œ∏ contained in the data:
```
I(Œ∏) = E[(‚àÇ log f(X; Œ∏)/‚àÇŒ∏)¬≤] = -E[‚àÇ¬≤ log f(X; Œ∏)/‚àÇŒ∏¬≤]
```

### Properties

**Additivity:** For iid observations:
```
I‚Çô(Œ∏) = nI(Œ∏)
```

**Information inequality:**
```
I(Œ∏) = -E[‚àÇ¬≤‚Ñì/‚àÇŒ∏¬≤]
```

**Reparameterization:** If œÜ = g(Œ∏):
```
I_œÜ(œÜ) = I_Œ∏(g‚Åª¬π(œÜ)) / (g'(g‚Åª¬π(œÜ)))¬≤
```

### Examples

**Normal N(Œº, œÉ¬≤) (Œº unknown, œÉ¬≤ known):**
```
I(Œº) = 1/œÉ¬≤
```

**Exponential Exp(Œª):**
```
I(Œª) = 1/Œª¬≤
```

**Bernoulli Ber(p):**
```
I(p) = 1/(p(1-p))
```

## 10.6 Cram√©r-Rao Lower Bound

### Theorem
For any unbiased estimator T of Œ∏:
```
Var(T) ‚â• 1/I(Œ∏)
```

**Multivariate version:** For unbiased estimator of Œ∏ ‚àà ‚Ñù·µà:
```
Cov(T) ‚™∞ I‚Åª¬π(Œ∏)
```

### Efficient Estimators
An estimator achieving the Cram√©r-Rao bound is called **efficient**.

**Theorem:** In exponential families, MLE is efficient.

### Rao-Blackwell Theorem
If T is unbiased for Œ∏ and S is sufficient, then:
```
T* = E[T|S]
```

is unbiased with Var(T*) ‚â§ Var(T).

## 10.7 Exponential Families

### Definition
A family of distributions is an **exponential family** if:
```
f(x; Œ∏) = h(x) exp{Œ∑(Œ∏)·µÄT(x) - A(Œ∏)}
```

where:
- T(x): sufficient statistic
- Œ∑(Œ∏): natural parameter  
- A(Œ∏): log partition function
- h(x): base measure

### Canonical Form
When Œ∑(Œ∏) = Œ∏, we have **canonical** or **natural** exponential family:
```
f(x; Œ∏) = h(x) exp{Œ∏·µÄT(x) - A(Œ∏)}
```

### Properties

**Sufficient statistic:** T(X) = ‚àë·µ¢T(X·µ¢)

**Mean and variance:**
```
E[T(X)] = ‚àáA(Œ∏)
Var(T(X)) = ‚àá¬≤A(Œ∏)
```

**Fisher information:** I(Œ∏) = ‚àá¬≤A(Œ∏)

**MLE efficiency:** MLE achieves Cram√©r-Rao bound.

### Examples

**Normal (both parameters unknown):**
```
T(x) = (x, x¬≤), Œ∏ = (Œº/œÉ¬≤, -1/(2œÉ¬≤))
```

**Poisson:**
```
T(x) = x, Œ∏ = log Œª, A(Œ∏) = e·∂ø
```

**Binomial:**
```
T(x) = x, Œ∏ = log(p/(1-p)), A(Œ∏) = n log(1 + e·∂ø)
```

## 10.8 Sufficiency and Completeness

### Sufficient Statistic
T(X) is **sufficient** for Œ∏ if the conditional distribution of X given T(X) doesn't depend on Œ∏.

**Factorization theorem:** T is sufficient if and only if:
```
f(x; Œ∏) = g(T(x); Œ∏)h(x)
```

### Minimal Sufficient Statistic
T is **minimal sufficient** if it's a function of every other sufficient statistic.

### Complete Statistic
T is **complete** if E[g(T)] = 0 for all Œ∏ implies g(T) = 0 a.s.

### Lehmann-Scheff√© Theorem
If T is complete and sufficient, and W is unbiased for Œ∏, then E[W|T] is the unique UMVU estimator.

## 10.9 Asymptotic Theory

### Regularity Conditions
1. **Identifiability:** Different Œ∏ give different distributions
2. **Common support:** Support doesn't depend on Œ∏  
3. **Differentiability:** Log-likelihood is twice differentiable
4. **Information conditions:** Fisher information exists and is finite

### Consistency of MLE
Under regularity conditions:
```
Œ∏ÃÇ‚Çô ‚Üí·µñ Œ∏‚ÇÄ
```

**Proof sketch:** Uses uniform law of large numbers and identification conditions.

### Asymptotic Normality
```
‚àön(Œ∏ÃÇ‚Çô - Œ∏‚ÇÄ) ‚áù N(0, I‚Åª¬π(Œ∏‚ÇÄ))
```

**Proof sketch:** Taylor expansion of score function around true parameter.

### Delta Method Applications
For smooth function g(Œ∏):
```
‚àön(g(Œ∏ÃÇ‚Çô) - g(Œ∏‚ÇÄ)) ‚áù N(0, ‚àág(Œ∏‚ÇÄ)·µÄI‚Åª¬π(Œ∏‚ÇÄ)‚àág(Œ∏‚ÇÄ))
```

## 10.10 Hypothesis Testing in Parametric Models

### Likelihood Ratio Test
For testing H‚ÇÄ: Œ∏ ‚àà Œò‚ÇÄ vs H‚ÇÅ: Œ∏ ‚àà Œò‚ÇÅ:
```
Œª = 2[‚Ñì(Œ∏ÃÇ) - ‚Ñì(Œ∏ÃÇ‚ÇÄ)]
```

**Wilks' theorem:** Under H‚ÇÄ:
```
Œª ‚áù œá¬≤_k
```

where k = dim(Œò) - dim(Œò‚ÇÄ).

### Wald Test
```
W = (Œ∏ÃÇ - Œ∏‚ÇÄ)·µÄI(Œ∏ÃÇ)(Œ∏ÃÇ - Œ∏‚ÇÄ) ‚áù œá¬≤_k
```

### Score Test (Lagrange Multiplier)
```
LM = S(Œ∏‚ÇÄ)·µÄI‚Åª¬π(Œ∏‚ÇÄ)S(Œ∏‚ÇÄ) ‚áù œá¬≤_k
```

### Relationships
All three tests are asymptotically equivalent under H‚ÇÄ:
```
LRT ‚âà Wald ‚âà Score
```

## 10.11 Confidence Intervals and Regions

### Wald Confidence Interval
```
Œ∏ÃÇ ¬± z_{Œ±/2}/‚àö(nI(Œ∏ÃÇ))
```

### Likelihood-based Intervals
```
{Œ∏ : 2[‚Ñì(Œ∏ÃÇ) - ‚Ñì(Œ∏)] ‚â§ œá¬≤‚ÇÅ,Œ±}
```

### Profile Likelihood
For parameter of interest œà = g(Œ∏):
```
‚Ñì‚Çö(œà) = max_{Œ∏:g(Œ∏)=œà} ‚Ñì(Œ∏)
```

**Profile likelihood interval:**
```
{œà : 2[‚Ñì‚Çö(œàÃÇ) - ‚Ñì‚Çö(œà)] ‚â§ œá¬≤‚ÇÅ,Œ±}
```

## 10.12 Bayesian vs Frequentist Paradigms

### Frequentist Approach
- Parameters are fixed unknown constants
- Probability refers to repeated sampling
- Confidence intervals have coverage probability

### Bayesian Approach  
- Parameters are random variables
- Probability represents degree of belief
- Credible intervals contain parameter with given probability

### Asymptotic Agreement
Under regularity conditions:
- Posterior ‚Üí N(Œ∏ÃÇ‚Çò‚Çó‚Çë, I‚Åª¬π(Œ∏ÃÇ‚Çò‚Çó‚Çë)/n)
- Credible intervals ‚âà confidence intervals

## 10.13 Model Selection

### Akaike Information Criterion (AIC)
```
AIC = -2‚Ñì(Œ∏ÃÇ) + 2k
```

where k is the number of parameters.

### Bayesian Information Criterion (BIC)
```
BIC = -2‚Ñì(Œ∏ÃÇ) + k log n
```

### Model Selection Procedure
1. Fit candidate models
2. Compute information criteria
3. Select model with smallest criterion

### Properties
- **AIC:** Asymptotically optimal for prediction
- **BIC:** Consistent for model selection

## 10.14 Robust Estimation

### M-estimators
Solve:
```
‚àë·µ¢ œà((x·µ¢ - Œ∏)/œÉ) = 0
```

where œà is chosen for robustness.

### Huber Estimator
```
œà(x) = {
  x           if |x| ‚â§ k
  k¬∑sign(x)   if |x| > k
}
```

### Breakdown Point
Fraction of outliers estimator can handle:
- Sample mean: 0%
- Sample median: 50%
- Huber estimator: depends on k

## 10.15 Bootstrap for Parametric Models

### Parametric Bootstrap
1. Estimate Œ∏ÃÇ from data
2. Generate bootstrap sample from f(¬∑; Œ∏ÃÇ)
3. Compute bootstrap statistic
4. Repeat to approximate sampling distribution

### Advantages
- More accurate than normal approximation
- Works with small samples
- Handles complex statistics

### Bootstrap Confidence Intervals
- Percentile method
- Bias-corrected methods  
- Bootstrap-t intervals

## 10.16 Computational Methods

### Newton-Raphson Algorithm
```
Œ∏‚ÅΩ·µè‚Å∫¬π‚Åæ = Œ∏‚ÅΩ·µè‚Åæ - H‚Åª¬π(Œ∏‚ÅΩ·µè‚Åæ)S(Œ∏‚ÅΩ·µè‚Åæ)
```

where H is Hessian matrix.

### Fisher Scoring
Replace Hessian with Fisher information:
```
Œ∏‚ÅΩ·µè‚Å∫¬π‚Åæ = Œ∏‚ÅΩ·µè‚Åæ + I‚Åª¬π(Œ∏‚ÅΩ·µè‚Åæ)S(Œ∏‚ÅΩ·µè‚Åæ)
```

### EM Algorithm
For models with missing data or latent variables:
- **E-step:** Compute expected complete data log-likelihood
- **M-step:** Maximize to update parameters

## 10.17 Multiparameter Extensions

### Score Vector
```
S(Œ∏) = (‚àÇ‚Ñì/‚àÇŒ∏‚ÇÅ, ..., ‚àÇ‚Ñì/‚àÇŒ∏‚Çñ)·µÄ
```

### Fisher Information Matrix
```
I(Œ∏) = E[S(Œ∏)S(Œ∏)·µÄ] = -E[‚àá¬≤‚Ñì(Œ∏)]
```

### Asymptotic Distribution
```
‚àön(Œ∏ÃÇ - Œ∏) ‚áù N(0, I‚Åª¬π(Œ∏))
```

### Marginal vs Conditional Inference
- **Marginal:** Ignore nuisance parameters
- **Conditional:** Condition on sufficient statistic for nuisance parameters

## Key Insights

1. **Efficiency:** MLE is asymptotically efficient in regular exponential families.

2. **Invariance:** MLE is invariant under reparameterization.

3. **Large Sample Theory:** Provides foundation for inference via normal approximations.

4. **Information:** Fisher information quantifies parameter estimability.

5. **Sufficiency:** Sufficient statistics contain all information about parameters.

## Common Pitfalls

1. **Regularity violations:** Boundary parameters, non-smooth likelihoods
2. **Finite sample bias:** MLE can be biased in small samples  
3. **Multiple maxima:** Likelihood may have local maxima
4. **Model misspecification:** Wrong parametric family
5. **Numerical issues:** Optimization algorithms may fail

## Practical Guidelines

### Model Specification
- Use domain knowledge and exploratory analysis
- Check distributional assumptions with diagnostic plots
- Consider multiple candidate models

### Estimation
- Check convergence of optimization algorithms
- Verify MLE is global maximum
- Use multiple starting values

### Inference
- Report both estimates and uncertainties
- Use appropriate inference method (Wald, LRT, Score)
- Check robustness to model assumptions

## Connections to Other Chapters

### To Chapter 4 (Expectation)
- Method of moments based on population moments
- Fisher information as expectation of score squares

### To Chapter 6 (Convergence)
- Consistency and asymptotic normality of estimators
- Delta method applications

### To Chapter 11 (Hypothesis Testing)
- Likelihood ratio, Wald, and Score tests
- Connection between confidence intervals and tests

### To Chapter 12 (Bayesian Inference)
- Maximum likelihood vs. MAP estimation
- Asymptotic equivalence of frequentist and Bayesian methods

This chapter provides the foundation for parametric statistical inference, connecting probability theory with practical estimation and testing procedures under specific distributional assumptions.