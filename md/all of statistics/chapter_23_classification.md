# Chapter 23: Classification - Mathematical Explanations

## Overview
Classification is the problem of predicting a discrete variable Y from another random variable X. This chapter covers fundamental classification methods, their theoretical foundations, and practical applications in supervised learning.

## 23.1 Introduction to Classification

### Problem Setup
Consider IID data (X‚ÇÅ, Y‚ÇÅ), ..., (X‚Çô, Y‚Çô) where:
- X·µ¢ = (X·µ¢‚ÇÅ, ..., X·µ¢‚Çì) ‚àà ùí≥ ‚äÇ ‚Ñù·µà is a d-dimensional feature vector
- Y·µ¢ ‚àà ùí¥ = {1, 2, ..., k} is the class label

**Goal:** Find a classification rule h: ùí≥ ‚Üí ùí¥ that predicts Y from X.

### Vocabulary Translation
| Statistics | Computer Science | Meaning |
|------------|------------------|---------|
| Classification | Supervised learning | Predicting discrete Y from X |
| Data | Training sample | (X‚ÇÅ, Y‚ÇÅ), ..., (X‚Çô, Y‚Çô) |
| Covariates | Features | The X·µ¢'s |
| Classifier | Hypothesis | Map h: ùí≥ ‚Üí ùí¥ |
| Estimation | Learning | Finding good classifier |

## 23.2 Risk and the Bayes Classifier

### Risk Function
The **risk** (or misclassification error) of classifier h is:
```
R(h) = P(h(X) ‚â† Y) = E[I(h(X) ‚â† Y)]
```

### Bayes Classifier
The **Bayes classifier** minimizes the risk:
```
h*(x) = argmax P(Y = j|X = x)
```

**Bayes risk:** R* = R(h*) (minimum possible risk)

### Posterior Probabilities
Let Œ∑‚±º(x) = P(Y = j|X = x) be the posterior probability.

**Properties:**
- ‚àë‚±º Œ∑‚±º(x) = 1
- Œ∑‚±º(x) ‚â• 0
- h*(x) = argmax Œ∑‚±º(x)

### Binary Classification
For k = 2 classes:
```
h*(x) = {
  1  if Œ∑‚ÇÅ(x) > 1/2
  0  if Œ∑‚ÇÅ(x) < 1/2
}
```

**Decision boundary:** {x : Œ∑‚ÇÅ(x) = 1/2}

## 23.3 Linear and Quadratic Classifiers

### Linear Discriminant Analysis (LDA)

**Assumptions:**
- X|Y = j ~ N(Œº‚±º, Œ£) (same covariance)
- P(Y = j) = œÄ‚±º

**Discriminant functions:**
```
Œ¥‚±º(x) = log œÄ‚±º - (1/2)Œº‚±º·µÄŒ£‚Åª¬πŒº‚±º + x·µÄŒ£‚Åª¬πŒº‚±º
```

**Classification rule:** h(x) = argmax Œ¥‚±º(x)

**Decision boundary between classes i and j:**
```
{x : Œ¥·µ¢(x) = Œ¥‚±º(x)}
```

This is linear in x, hence "linear" discriminant analysis.

### Sample-based LDA
Replace population parameters with sample estimates:
- ŒºÃÇ‚±º = (1/n‚±º) ‚àë·µ¢:Y·µ¢=j X·µ¢
- Œ£ÃÇ = (1/(n-k)) ‚àë‚±º ‚àë·µ¢:Y·µ¢=j (X·µ¢ - ŒºÃÇ‚±º)(X·µ¢ - ŒºÃÇ‚±º)·µÄ
- œÄÃÇ‚±º = n‚±º/n

### Quadratic Discriminant Analysis (QDA)

**Assumption:** X|Y = j ~ N(Œº‚±º, Œ£‚±º) (different covariances)

**Discriminant functions:**
```
Œ¥‚±º(x) = log œÄ‚±º - (1/2)log|Œ£‚±º| - (1/2)(x - Œº‚±º)·µÄŒ£‚±º‚Åª¬π(x - Œº‚±º)
```

**Decision boundaries:** Quadratic in x

### Bias-Variance Trade-off
- **LDA:** Lower variance, higher bias (assumes equal covariances)
- **QDA:** Higher variance, lower bias (more flexible)

## 23.4 Logistic Regression

### Model
For binary classification:
```
log(P(Y = 1|X = x)/(P(Y = 0|X = x))) = Œ≤‚ÇÄ + Œ≤·µÄx
```

**Probability:**
```
P(Y = 1|X = x) = exp(Œ≤‚ÇÄ + Œ≤·µÄx)/(1 + exp(Œ≤‚ÇÄ + Œ≤·µÄx))
```

### Multinomial Logistic Regression
For k classes, use k-1 log-odds:
```
log(P(Y = j|X = x)/P(Y = k|X = x)) = Œ≤‚ÇÄ‚±º + Œ≤‚±º·µÄx, j = 1, ..., k-1
```

**Probabilities:**
```
P(Y = j|X = x) = exp(Œ≤‚ÇÄ‚±º + Œ≤‚±º·µÄx)/(1 + ‚àë‚Çó‚Çå‚ÇÅ·µè‚Åª¬π exp(Œ≤‚ÇÄ‚Çó + Œ≤‚Çó·µÄx))
```

### Maximum Likelihood Estimation
**Log-likelihood:**
```
‚Ñì(Œ≤) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø ‚àë‚±º‚Çå‚ÇÅ·µè Y·µ¢‚±º log P(Y = j|X·µ¢, Œ≤)
```

where Y·µ¢‚±º = I(Y·µ¢ = j).

**No closed form:** Use iterative methods (Newton-Raphson, IRLS)

## 23.5 Classification Trees

### Tree Structure
- **Internal nodes:** Tests on features
- **Leaves:** Class predictions
- **Path:** Sequence of tests leading to prediction

### Growing Trees

**Recursive binary splitting:**
1. Find best split (feature + threshold)
2. Split data into two subsets
3. Repeat recursively on each subset

**Splitting criteria:**
- **Misclassification rate:** ‚àë‚±º I(ƒµ ‚â† j)pÃÇ‚±º
- **Gini index:** ‚àë‚±º pÃÇ‚±º(1 - pÃÇ‚±º) = 1 - ‚àë‚±º pÃÇ‚±º¬≤
- **Entropy:** -‚àë‚±º pÃÇ‚±º log pÃÇ‚±º

where pÃÇ‚±º = proportion of class j in node.

### Pruning
Large trees overfit. Use **cost-complexity pruning:**

**Cost-complexity criterion:**
```
C‚Çê(T) = ‚àë‚Çú N‚Çú Q‚Çú + Œ±|T|
```

where:
- N‚Çú = number of observations in leaf t
- Q‚Çú = misclassification rate in leaf t  
- |T| = number of leaves
- Œ± = complexity parameter

**Procedure:**
1. Grow large tree T‚ÇÄ
2. For each Œ±, find optimal subtree T‚Çê
3. Use cross-validation to choose Œ±

### Advantages and Disadvantages
**Advantages:**
- Easy to interpret
- Handles mixed data types
- Automatic feature selection
- Captures interactions

**Disadvantages:**
- High variance
- Biased toward features with more levels
- Difficulty capturing linear relationships

## 23.6 Nearest Neighbors

### k-Nearest Neighbors (k-NN)

**Algorithm:**
1. Find k nearest neighbors to x in training set
2. Classify x as majority class among k neighbors

**Distance metrics:**
- **Euclidean:** ||x - x·µ¢||‚ÇÇ = ‚àö‚àë‚±º(x‚±º - x·µ¢‚±º)¬≤
- **Manhattan:** ||x - x·µ¢||‚ÇÅ = ‚àë‚±º|x‚±º - x·µ¢‚±º|
- **Minkowski:** ||x - x·µ¢||‚Çö = (‚àë‚±º|x‚±º - x·µ¢‚±º|·µñ)^(1/p)

### Theoretical Properties

**Consistency:** As n ‚Üí ‚àû and k ‚Üí ‚àû with k/n ‚Üí 0:
```
R(ƒ•‚Çñ) ‚Üí R* (Bayes risk)
```

**Curse of dimensionality:** Performance degrades in high dimensions
- All points become equidistant
- Need exponentially more data

### Choosing k
- **Small k:** Low bias, high variance
- **Large k:** High bias, low variance
- **Typical choice:** k = ‚àön

**Cross-validation:** Common method for selecting k

## 23.7 Support Vector Machines

### Linear SVM (Separable Case)

**Goal:** Find hyperplane that separates classes with maximum margin

### Linear SVM (Separable Case)

**Goal:** Find hyperplane that separates classes with maximum margin

**Hyperplane:** w·µÄx + b = 0

**Margin:** Distance from hyperplane to nearest point = 1/||w||

**Optimization problem:**
```
minimize (1/2)||w||¬≤
subject to y·µ¢(w·µÄx·µ¢ + b) ‚â• 1, i = 1, ..., n
```

**Solution:** Only depends on **support vectors** (points on margin boundary)

### Linear SVM (Non-separable Case)

**Soft margin SVM:** Allow some misclassification
```
minimize (1/2)||w||¬≤ + C‚àë·µ¢Œæ·µ¢
subject to y·µ¢(w·µÄx·µ¢ + b) ‚â• 1 - Œæ·µ¢, Œæ·µ¢ ‚â• 0
```

where Œæ·µ¢ are **slack variables** and C controls trade-off between margin and violations.

### Kernel SVM

**Key insight:** Transform data to higher-dimensional space where linear separation possible

**Kernel trick:** Instead of computing œÜ(x) explicitly, use kernel function:
```
K(x, x') = œÜ(x)·µÄœÜ(x')
```

**Decision function:**
```
f(x) = ‚àë·µ¢ Œ±·µ¢y·µ¢K(x·µ¢, x) + b
```

### Common Kernels

**Linear:** K(x, x') = x·µÄx'

**Polynomial:** K(x, x') = (Œ≥x·µÄx' + r)·µà

**RBF (Gaussian):** K(x, x') = exp(-Œ≥||x - x'||¬≤)

**Sigmoid:** K(x, x') = tanh(Œ≥x·µÄx' + r)

### Properties
- **Maximum margin:** Unique solution (if exists)
- **Sparse:** Only support vectors matter
- **Kernel trick:** Handles nonlinear boundaries
- **Regularization:** C parameter controls overfitting

## 23.8 Neural Networks

### Single Layer Perceptron
**Model:**
```
f(x) = œÉ(w‚ÇÄ + ‚àë‚±ºw‚±ºx‚±º)
```

where œÉ is activation function (e.g., sigmoid, tanh, ReLU).

**Limitation:** Can only learn linearly separable functions

### Multi-layer Perceptron (MLP)

**Architecture:**
- **Input layer:** Features x‚ÇÅ, ..., x‚Çì
- **Hidden layers:** Transformed features
- **Output layer:** Class probabilities

**Forward propagation:**
```
z‚ÇÅ = W‚ÇÅx + b‚ÇÅ
a‚ÇÅ = œÉ(z‚ÇÅ)
‚ãÆ
z‚Çó = W‚Çóa‚Çó‚Çã‚ÇÅ + b‚Çó
≈∑ = softmax(z‚Çó)
```

### Backpropagation
**Algorithm for computing gradients:**
1. Forward pass: Compute predictions
2. Backward pass: Compute gradients using chain rule
3. Update weights: w·µ¢‚±º ‚Üê w·µ¢‚±º - Œ∑ ‚àÇL/‚àÇw·µ¢‚±º

**Gradient computation:**
```
‚àÇL/‚àÇw·µ¢‚±º = ‚àÇL/‚àÇz‚±º ¬∑ ‚àÇz‚±º/‚àÇw·µ¢‚±º = Œ¥‚±º ¬∑ a·µ¢
```

### Universal Approximation
**Theorem:** MLP with one hidden layer can approximate any continuous function on compact set arbitrarily well (with enough hidden units).

**Practical limitation:** May need exponentially many hidden units

## 23.9 Ensemble Methods

### Bootstrap Aggregating (Bagging)

**Algorithm:**
1. Generate B bootstrap samples
2. Train classifier on each sample
3. Combine predictions by voting

**Variance reduction:** Averaging reduces variance without increasing bias

**Random Forests:** Bagging + random feature selection
- At each split, consider only subset of features
- Further reduces correlation between trees

### Boosting

**Key idea:** Sequentially build weak learners, focusing on previously misclassified examples

**AdaBoost algorithm:**
1. Initialize weights: w·µ¢ = 1/n
2. For m = 1, ..., M:
   - Train classifier h‚Çò on weighted data
   - Compute error: Œµ‚Çò = ‚àë·µ¢ w·µ¢I(h‚Çò(x·µ¢) ‚â† y·µ¢)
   - Compute Œ±‚Çò = (1/2)log((1-Œµ‚Çò)/Œµ‚Çò)
   - Update weights: w·µ¢ ‚Üê w·µ¢exp(Œ±‚ÇòI(h‚Çò(x·µ¢) ‚â† y·µ¢))
   - Normalize weights

**Final classifier:**
```
H(x) = sign(‚àë‚Çò Œ±‚Çòh‚Çò(x))
```

### Gradient Boosting
**Functional gradient descent:** Fit new learner to residuals

**Algorithm:**
1. Initialize f‚ÇÄ(x) = argmin ‚àë·µ¢ L(y·µ¢, Œ≥)
2. For m = 1, ..., M:
   - Compute residuals: r·µ¢ = -‚àÇL(y·µ¢, f(x·µ¢))/‚àÇf(x·µ¢)
   - Fit h‚Çò(x) to residuals
   - Find Œ≥‚Çò = argmin ‚àë·µ¢ L(y·µ¢, f‚Çò‚Çã‚ÇÅ(x·µ¢) + Œ≥h‚Çò(x·µ¢))
   - Update: f‚Çò(x) = f‚Çò‚Çã‚ÇÅ(x) + Œ≥‚Çòh‚Çò(x)

## 23.10 Model Assessment and Selection

### Training vs Test Error
**Training error:** Error on data used to fit model
**Test error:** Error on independent test data

**Key insight:** Training error underestimates true error (overfitting)

### Cross-Validation

**k-fold CV:**
1. Divide data into k folds
2. For each fold:
   - Train on k-1 folds
   - Test on remaining fold
3. Average test errors

**Leave-one-out CV:** Special case with k = n

### Model Selection Criteria

**AIC:** 2k - 2ln(LÃÇ)
**BIC:** k ln(n) - 2ln(LÃÇ)

where k = number of parameters, LÃÇ = maximum likelihood.

### Performance Metrics

**Confusion Matrix:**
|           | Predicted |          |
|-----------|-----------|----------|
| **Actual**| Positive  | Negative |
| Positive  | TP        | FN       |
| Negative  | FP        | TN       |

**Metrics:**
- **Accuracy:** (TP + TN)/(TP + TN + FP + FN)
- **Precision:** TP/(TP + FP)
- **Recall (Sensitivity):** TP/(TP + FN)
- **Specificity:** TN/(TN + FP)
- **F1-score:** 2 √ó (Precision √ó Recall)/(Precision + Recall)

### ROC Curves
**ROC curve:** Plot of True Positive Rate vs False Positive Rate

**AUC:** Area Under the ROC Curve
- AUC = 0.5: Random classifier
- AUC = 1.0: Perfect classifier

## 23.11 Statistical Learning Theory

### PAC Learning Framework
**Probably Approximately Correct (PAC) learning:**

A concept class is PAC-learnable if there exists algorithm that, for any Œµ > 0, Œ¥ > 0:
- Uses polynomial number of samples and computation time
- With probability ‚â• 1-Œ¥, outputs hypothesis with error ‚â§ Œµ

### VC Dimension

**Definition:** VC dimension of hypothesis class ‚Ñã is largest set size that ‚Ñã can shatter.

**Shattering:** ‚Ñã shatters set S if for every labeling of S, some h ‚àà ‚Ñã achieves zero error.

**Examples:**
- Linear classifiers in ‚Ñù·µà: VC dimension = d + 1
- k-NN: VC dimension = ‚àû

### Generalization Bounds

**VC inequality:** With probability 1-Œ¥:
```
R(h) ‚â§ RÃÇ(h) + ‚àö[(2/n)(VC(‚Ñã) + log(2/Œ¥))]
```

where RÃÇ(h) is empirical risk.

**Rademacher complexity:** Sharper bounds using data-dependent measures

### Structural Risk Minimization
Balance empirical risk and model complexity:
```
minimize RÃÇ(h) + ‚àö[complexity penalty]
```

## 23.12 Feature Selection and Dimensionality Reduction

### Filter Methods
**Univariate selection:** Score each feature independently
- Chi-square test for categorical features
- Mutual information
- Correlation with target

### Wrapper Methods
**Forward selection:**
1. Start with empty set
2. Add feature that most improves performance
3. Stop when no improvement

**Backward elimination:** Start with all features, remove worst

### Embedded Methods
**L1 regularization (Lasso):** Automatically selects features
**Tree-based:** Features used in tree splits are selected

### Principal Component Analysis (PCA)
**Goal:** Find low-dimensional representation preserving variance

**Steps:**
1. Center data: XÃÉ = X - XÃÑ
2. Compute covariance: C = XÃÉ·µÄXÃÉ/(n-1)
3. Find eigenvectors of C
4. Project onto top k eigenvectors

**Linear Discriminant Analysis (LDA):** Finds projection maximizing between-class separation

## 23.13 Imbalanced Data

### Problem
When classes have very different frequencies:
- Algorithms biased toward majority class
- Minority class often more important

### Solutions

**Resampling:**
- **Undersampling:** Remove majority class examples
- **Oversampling:** Duplicate minority class examples
- **SMOTE:** Generate synthetic minority examples

**Cost-sensitive learning:** Assign different costs to misclassification types

**Threshold adjustment:** Adjust decision threshold based on class frequencies

### Evaluation Metrics
Standard accuracy misleading for imbalanced data:
- Use precision, recall, F1-score
- Area under precision-recall curve
- Matthews correlation coefficient

## 23.14 Multi-class Classification

### One-vs-Rest (OvR)
Train k binary classifiers:
- Classifier j: class j vs all others
- Prediction: argmax f‚±º(x)

### One-vs-One (OvO)
Train k(k-1)/2 binary classifiers:
- One for each pair of classes
- Prediction: majority vote

### Direct Multi-class
Some algorithms naturally handle multiple classes:
- Multinomial logistic regression
- Decision trees
- Neural networks

## 23.15 Online Learning

### Setting
Data arrives sequentially: (x‚ÇÅ, y‚ÇÅ), (x‚ÇÇ, y‚ÇÇ), ...

**Goal:** Make prediction ≈∑‚Çú before seeing y‚Çú

### Perceptron Algorithm
**Update rule:** If mistake at time t:
```
w‚Çú‚Çä‚ÇÅ = w‚Çú + y‚Çúx‚Çú
```

**Mistake bound:** Number of mistakes ‚â§ R¬≤/Œ≥¬≤
where R = max ||x‚Çú||, Œ≥ = margin

### Gradient Descent
**Update:** w‚Çú‚Çä‚ÇÅ = w‚Çú - Œ∑‚Çú‚àáL(y‚Çú, w‚Çú·µÄx‚Çú)

**Regret bound:** O(‚àöT) for convex losses

## 23.16 Semi-supervised Learning

### Setting
Small amount of labeled data + large amount of unlabeled data

### Self-training
1. Train classifier on labeled data
2. Predict labels for unlabeled data
3. Add high-confidence predictions to training set
4. Repeat

### Co-training
**Requirements:** Features can be split into two views
1. Train two classifiers on different feature views
2. Each classifier labels examples for the other
3. Add high-confidence predictions to training set

### Graph-based Methods
**Assumption:** Nearby points likely to have same label
- Build graph connecting similar points
- Propagate labels through graph

## 23.17 Deep Learning for Classification

### Convolutional Neural Networks (CNNs)
**Architecture:**
- **Convolutional layers:** Local feature detection
- **Pooling layers:** Dimensionality reduction
- **Fully connected layers:** Final classification

**Advantages:** 
- Translation invariance
- Parameter sharing
- Hierarchical feature learning

### Transfer Learning
**Idea:** Use pre-trained network as feature extractor
1. Take network trained on large dataset (e.g., ImageNet)
2. Remove final layer
3. Add new classifier for target task
4. Fine-tune on target data

### Regularization Techniques
**Dropout:** Randomly set fraction of neurons to zero during training
**Batch normalization:** Normalize inputs to each layer
**Data augmentation:** Generate variations of training data

## 23.18 Practical Considerations

### Data Preprocessing
**Standardization:** Scale features to have mean 0, variance 1
**Normalization:** Scale to [0,1] range
**Categorical encoding:** One-hot, ordinal, target encoding

### Hyperparameter Tuning
**Grid search:** Try all combinations of parameter values
**Random search:** Sample parameter values randomly
**Bayesian optimization:** Use probabilistic models to guide search

### Model Interpretation
**Feature importance:** Which features matter most?
**Partial dependence plots:** How does each feature affect predictions?
**SHAP values:** Unified approach to feature attribution

### Computational Considerations
**Scalability:** How does algorithm scale with n, d?
**Memory requirements:** Can algorithm handle large datasets?
**Parallelization:** Can computation be distributed?

## 23.19 Modern Developments

### AutoML
**Goal:** Automate machine learning pipeline
- Feature engineering
- Algorithm selection
- Hyperparameter tuning

### Federated Learning
**Setting:** Training across multiple devices/organizations
- Data remains local
- Only model updates shared
- Privacy preservation

### Adversarial Examples
**Problem:** Small perturbations can fool classifiers
**Defense:** Adversarial training, certified defenses

## Key Insights

1. **No Free Lunch:** No single classifier dominates all problems
2. **Bias-Variance Trade-off:** Central theme in model selection
3. **Overfitting:** Major concern, especially with complex models
4. **Feature Engineering:** Often more important than algorithm choice
5. **Evaluation:** Proper assessment crucial for real-world deployment

## Common Pitfalls

1. **Data leakage:** Using future information to predict past
2. **Selection bias:** Non-representative training data
3. **Overfitting to validation set:** Repeated model tuning
4. **Ignoring class imbalance:** Misleading accuracy metrics
5. **Correlation vs causation:** Classification ‚â† causal inference

## Comparison of Methods

| Method | Interpretability | Flexibility | Scalability | Assumptions |
|--------|------------------|-------------|-------------|-------------|
| Logistic Regression | High | Low | High | Linear boundaries |
| Decision Trees | High | Medium | Medium | None |
| Random Forest | Medium | High | High | None |
| SVM | Low | High | Medium | None |
| Neural Networks | Low | Very High | High | Large data |
| k-NN | Medium | High | Poor | Local smoothness |

## Conclusion

Classification is a rich field connecting statistics, computer science, and machine learning. The choice of method depends on:
- Data size and dimensionality
- Interpretability requirements
- Computational constraints
- Domain knowledge

Modern practice often involves ensemble methods combining multiple approaches for robust performance.